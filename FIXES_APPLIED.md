# è®­ç»ƒé—®é¢˜ä¿®å¤è¯´æ˜Ž

## ä¿®å¤æ—¥æœŸ
2025å¹´12æœˆ9æ—¥

## å‘çŽ°çš„é—®é¢˜åŠä¿®å¤æ–¹æ¡ˆ

### 1. ðŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±å¯¼è‡´è®­ç»ƒå¤±è´¥

**é”™è¯¯ä¿¡æ¯ï¼š**
```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

**æ ¹æœ¬åŽŸå› ï¼š**
åœ¨ `mem_adapter_only.py` çš„ `forward()` æ–¹æ³•ä¸­ï¼ˆç¬¬385-387è¡Œé™„è¿‘ï¼‰ï¼ŒOCRç¼–ç å™¨çš„è°ƒç”¨è¢« `with torch.no_grad():` åŒ…è£¹ï¼Œå¯¼è‡´æ¢¯åº¦è®¡ç®—å›¾æ–­è£‚ã€‚

**ä¿®å¤æ–¹æ¡ˆï¼š**
- âœ… ç§»é™¤äº† `with torch.no_grad():` ä¸Šä¸‹æ–‡ç®¡ç†å™¨
- âœ… æ·»åŠ è¯¦ç»†æ³¨é‡Šè¯´æ˜Žä¸ºä½•éœ€è¦ä¿æŒæ¢¯åº¦æµåŠ¨
- **å…³é”®åŽŸç†**ï¼šè™½ç„¶OCRç¼–ç å™¨çš„å‚æ•°è¢«å†»ç»“ï¼ˆ`requires_grad=False`ï¼‰ï¼Œä½†æ¢¯åº¦å¿…é¡»èƒ½å¤Ÿæµè¿‡è¯¥æ¨¡å—æ‰èƒ½åå‘ä¼ æ’­åˆ°å¯è®­ç»ƒçš„ `proj` å±‚ã€‚`torch.no_grad()` ä¼šå®Œå…¨æ–­å¼€è®¡ç®—å›¾ï¼Œå¯¼è‡´lossæ²¡æœ‰grad_fnã€‚

**ä¿®å¤ä½ç½®ï¼š**
- `mem_adapter_only.py` ç¬¬383-402è¡Œ

---

### 2. âš ï¸ é…ç½®éªŒè¯é—®é¢˜

**è­¦å‘Šä¿¡æ¯ï¼š**
```
WARNING:root:base_model_name is empty, model may not initialize correctly
WARNING:root:ocr_model_name is empty, model may not initialize correctly
```

**æ ¹æœ¬åŽŸå› ï¼š**
1. `MEMConfig.__init__()` ä¸­ä½¿ç”¨ `logging.warning()` è€Œéžå¼‚å¸¸æŠ›å‡º
2. åœ¨ `from_pretrained()` åŠ è½½å·²ä¿å­˜æ¨¡åž‹æ—¶ä¹Ÿä¼šè§¦å‘ä¸å¿…è¦çš„è­¦å‘Š

**ä¿®å¤æ–¹æ¡ˆï¼š**
- âœ… åœ¨ `MEMModel.__init__()` ä¸­æ·»åŠ ä¸¥æ ¼çš„å‚æ•°éªŒè¯
- âœ… ä½¿ç”¨ `ValueError` è€Œéž warningï¼Œç¡®ä¿é…ç½®é”™è¯¯ç«‹å³è¢«å‘çŽ°
- âœ… ä¿æŒ `MEMConfig` çš„çµæ´»æ€§ï¼Œå…è®¸ `from_pretrained()` æ—¶å‚æ•°ä¸ºç©º

**ä¿®å¤ä½ç½®ï¼š**
- `mem_adapter_only.py` ç¬¬21-43è¡Œï¼ˆMEMConfigï¼‰
- `mem_adapter_only.py` ç¬¬59-75è¡Œï¼ˆMEMModel.__init__ï¼‰

---

### 3. âš ï¸ Transformersç‰ˆæœ¬ä¸å…¼å®¹

**è­¦å‘Šä¿¡æ¯ï¼š**
```
WARNING: Incompatible transformers version detected!
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0
```

**æ ¹æœ¬åŽŸå› ï¼š**
DeepSeek-OCRæ¨¡åž‹åœ¨è®¾è®¡æ—¶ä½¿ç”¨äº† transformers 4.30-4.48 ä¹‹é—´çš„ç‰¹æ€§ï¼Œä¸Žå½“å‰ç‰ˆæœ¬ 4.57.3 ä¸å®Œå…¨å…¼å®¹ã€‚

**å½±å“è¯„ä¼°ï¼š**
- ðŸŸ¡ è­¦å‘Šæ€§é—®é¢˜ï¼Œä¸ä¼šç›´æŽ¥å¯¼è‡´è®­ç»ƒå¤±è´¥
- å¯èƒ½å½±å“ï¼šæ¨¡åž‹åŠ è½½æ—¶çš„æŸäº›è¡Œä¸ºã€æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§

**å»ºè®®æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰ï¼š**
```bash
# å¦‚æžœé‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œå¯ä»¥é™çº§transformersç‰ˆæœ¬
pip install 'transformers>=4.30.0,<4.48.0'
# æŽ¨èç‰ˆæœ¬ï¼štransformers==4.47.0
```

**æ³¨æ„ï¼š** å¦‚æžœå½“å‰ç‰ˆæœ¬å·¥ä½œæ­£å¸¸ï¼Œæ— éœ€é™çº§ã€‚

---

### 4. âš ï¸ æ¨¡åž‹ç±»åž‹ä¸åŒ¹é…è­¦å‘Š

**è­¦å‘Šä¿¡æ¯ï¼š**
```
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR.
This is not supported for all configurations of models and can yield errors.
```

**æ ¹æœ¬åŽŸå› ï¼š**
DeepSeek-OCR æ¨¡åž‹åŸºäºŽ deepseek_vl_v2 æž¶æž„ï¼Œä½†è¢«æ³¨å†Œä¸ºä¸åŒçš„æ¨¡åž‹ç±»åž‹ã€‚è¿™æ˜¯æ¨¡åž‹è®¾è®¡æ–¹çš„å·²çŸ¥æƒ…å†µã€‚

**å½±å“è¯„ä¼°ï¼š**
- ðŸŸ¢ ä»…ä¸ºä¿¡æ¯æ€§è­¦å‘Š
- ä¸å½±å“æ¨¡åž‹åŠŸèƒ½å’Œè®­ç»ƒ

**å¤„ç†æ–¹å¼ï¼š**
- æ— éœ€ä¿®å¤ï¼Œè¿™æ˜¯æ­£å¸¸çš„è­¦å‘Š
- å¯ä»¥é€šè¿‡è®¾ç½® `logging` çº§åˆ«æ¥æŠ‘åˆ¶

---

### 5. âš ï¸ Tokenizerå‚æ•°å·²å¼ƒç”¨

**è­¦å‘Šä¿¡æ¯ï¼š**
```
FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 
for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
```

**ä¿®å¤æ–¹æ¡ˆï¼š**
- âœ… åœ¨ `AdapterOnlyTrainer.__init__()` ä¸­è‡ªåŠ¨è½¬æ¢å‚æ•°
- âœ… å‘åŽå…¼å®¹ï¼šå¦‚æžœä¼ å…¥ `tokenizer`ï¼Œè‡ªåŠ¨é‡å‘½åä¸º `processing_class`

**ä¿®å¤ä½ç½®ï¼š**
- `trainer/adapter_only_trainer.py` ç¬¬165-178è¡Œ

---

### 6. â„¹ï¸ æœªåˆå§‹åŒ–æƒé‡ä¿¡æ¯

**ä¿¡æ¯ï¼š**
```
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint:
['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task.
```

**è¯´æ˜Žï¼š**
- ðŸŸ¢ è¿™æ˜¯æ­£å¸¸è¡Œä¸º
- `position_ids` æ˜¯ä¸€ä¸ªç¼“å†²åŒºï¼ˆbufferï¼‰ï¼Œä¸æ˜¯å¯è®­ç»ƒå‚æ•°
- ä¼šåœ¨æ¨¡åž‹é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨åˆå§‹åŒ–

---

## ä¿®å¤åŽçš„è®­ç»ƒæµç¨‹

### æ­£ç¡®çš„æ¢¯åº¦æµåŠ¨è·¯å¾„

```
input_ids (å†»ç»“)
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
embedding layer (å†»ç»“ï¼Œä½†æœ‰grad_fn)
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
inputs_embeds
    â†“
åŽ†å²æ–‡æœ¬ â†’ æ¸²æŸ“å›¾åƒ â†’ OCRç¼–ç å™¨ (å†»ç»“ï¼Œä½†æœ‰grad_fn)
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
vision_features
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
proj å±‚ (âœ… å¯è®­ç»ƒï¼Œrequires_grad=True)
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
vision_embeds
    â†“
æ‹¼æŽ¥ + padding
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
base_llm (å†»ç»“ï¼Œä½†æœ‰grad_fn)
    â†“ (æ¢¯åº¦å¯æµåŠ¨)
logits â†’ loss (âœ… æœ‰grad_fn)
```

### å…³é”®åŽŸåˆ™

1. **å‚æ•°å†»ç»“ â‰  æ¢¯åº¦é˜»æ–­**
   - `param.requires_grad = False`: å‚æ•°ä¸æ›´æ–°ï¼Œä½†æ¢¯åº¦å¯ä»¥æµè¿‡
   - `torch.no_grad()`: å®Œå…¨æ–­å¼€è®¡ç®—å›¾ï¼Œæ¢¯åº¦æ— æ³•æµåŠ¨
   - `model.eval()`: æ”¹å˜dropout/batchnormè¡Œä¸ºï¼Œä½†ä¸å½±å“æ¢¯åº¦

2. **æ­£ç¡®çš„å†»ç»“æ–¹å¼**
   ```python
   # âœ… æ­£ç¡®ï¼šåªå†»ç»“å‚æ•°
   for param in model.parameters():
       param.requires_grad = False
   model.train()  # ä¿æŒtrainæ¨¡å¼
   
   # âŒ é”™è¯¯ï¼šæ–­å¼€æ¢¯åº¦
   model.eval()  # ä¼šå½±å“æŸäº›å±‚çš„è¡Œä¸º
   with torch.no_grad():  # å®Œå…¨æ–­å¼€è®¡ç®—å›¾
       output = model(input)
   ```

3. **è®­ç»ƒæ¨¡å¼è¯´æ˜Ž**
   - å†»ç»“æ¨¡å—ä»éœ€ä¿æŒ `train()` æ¨¡å¼ä»¥ä¼ é€’æ¢¯åº¦
   - åªåœ¨æŽ¨ç†æ—¶ä½¿ç”¨ `eval()` æ¨¡å¼

---

## éªŒè¯ä¿®å¤æ•ˆæžœ

### é¢„æœŸçš„æ­£å¸¸è¡Œä¸º

1. **å‚æ•°ç»Ÿè®¡åº”è¯¥æ˜¾ç¤ºï¼š**
   ```
   trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%
   Detailed breakdown:
     base_llm_model trainable params: 0 (should be 0)
     ocr_embed trainable params: 0 (should be 0)
     proj trainable params: 18,882,560
   ```

2. **è®­ç»ƒåº”è¯¥æ­£å¸¸å¼€å§‹ï¼š**
   - ä¸å†å‡ºçŽ° `RuntimeError: element 0 of tensors does not require grad`
   - lossèƒ½å¤Ÿæ­£å¸¸è®¡ç®—å’Œåå‘ä¼ æ’­
   - æ¢¯åº¦æ›´æ–°åªåº”ç”¨åˆ°projå±‚

3. **è­¦å‘Šä¿¡æ¯ï¼š**
   - âœ… `base_model_name is empty` è­¦å‘Šæ¶ˆå¤±
   - âœ… `tokenizer is deprecated` è­¦å‘Šæ¶ˆå¤±
   - ðŸŸ¡ transformersç‰ˆæœ¬è­¦å‘Šä»å­˜åœ¨ï¼ˆå¯æŽ¥å—ï¼‰
   - ðŸŸ¡ æ¨¡åž‹ç±»åž‹ä¸åŒ¹é…è­¦å‘Šä»å­˜åœ¨ï¼ˆå¯æŽ¥å—ï¼‰

---

## å¦‚ä½•éªŒè¯ä¿®å¤

### 1. å¿«é€ŸéªŒè¯ï¼ˆæŽ¨èï¼‰

```python
# åœ¨Pythonä¸­æµ‹è¯•
from mem_adapter_only import MEMModel, MEMConfig
import torch

config = MEMConfig(
    base_model_name="path/to/qwen",
    ocr_model_name="path/to/deepseek-ocr"
)
model = MEMModel(config)

# éªŒè¯æ¢¯åº¦æµåŠ¨
model.train()
dummy_input = torch.randint(0, 1000, (1, 100))
dummy_labels = torch.randint(0, 1000, (1, 100))

outputs = model(
    input_ids=dummy_input,
    attention_mask=torch.ones_like(dummy_input),
    labels=dummy_labels
)

# æ£€æŸ¥lossæ˜¯å¦æœ‰æ¢¯åº¦
assert outputs.loss.requires_grad, "Loss should have gradient!"
print("âœ… æ¢¯åº¦æµåŠ¨æ­£å¸¸ï¼")

# åå‘ä¼ æ’­æµ‹è¯•
outputs.loss.backward()
print("âœ… åå‘ä¼ æ’­æˆåŠŸï¼")

# æ£€æŸ¥projå±‚æœ‰æ¢¯åº¦
for name, param in model.named_parameters():
    if 'proj' in name and param.requires_grad:
        assert param.grad is not None, f"{name} should have gradient!"
print("âœ… projå±‚æ¢¯åº¦æ­£å¸¸ï¼")
```

### 2. å®Œæ•´è®­ç»ƒéªŒè¯

```bash
# é‡æ–°è¿è¡Œè®­ç»ƒ
cd /path/to/OCR-MEM
bash run_training.sh
```

è§‚å¯Ÿè¾“å‡ºï¼š
- âœ… è®­ç»ƒåº”è¯¥èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨
- âœ… ç¬¬ä¸€ä¸ªbatchåŽlossåº”è¯¥æ­£å¸¸æ›´æ–°
- âœ… wandbåº”è¯¥è®°å½•åˆ°æ­£å¸¸çš„lossæ›²çº¿

---

## æ€»ç»“

### ä¿®å¤çš„æ–‡ä»¶
1. `mem_adapter_only.py` - æ ¸å¿ƒä¿®å¤
2. `trainer/adapter_only_trainer.py` - è­¦å‘Šä¿®å¤

### ä¿®å¤çš„é—®é¢˜
- ðŸ”´ æ¢¯åº¦æ¶ˆå¤±ï¼ˆè®­ç»ƒå¤±è´¥çš„æ ¹æœ¬åŽŸå› ï¼‰âœ… å·²ä¿®å¤
- âš ï¸ é…ç½®éªŒè¯ä¸è¶³ âœ… å·²ä¿®å¤
- âš ï¸ Tokenizerå¼ƒç”¨è­¦å‘Š âœ… å·²ä¿®å¤
- ðŸŸ¡ Transformersç‰ˆæœ¬è­¦å‘Šï¼ˆå¯æŽ¥å—ï¼‰
- ðŸŸ¡ æ¨¡åž‹ç±»åž‹è­¦å‘Šï¼ˆå¯æŽ¥å—ï¼‰

### ä¸éœ€è¦ä¿®å¤çš„
- DeepSeek-OCRæ¨¡åž‹ç±»åž‹è­¦å‘Šï¼ˆæ­£å¸¸è¡Œä¸ºï¼‰
- position_idsæœªåˆå§‹åŒ–ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–ï¼‰

---

## åŽç»­å»ºè®®

1. **ç›‘æŽ§è®­ç»ƒæŒ‡æ ‡**
   - è§‚å¯Ÿlossæ˜¯å¦æ­£å¸¸ä¸‹é™
   - æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æ˜¯å¦åˆç†
   - éªŒè¯checkpointä¿å­˜æ˜¯å¦æ­£å¸¸

2. **æ€§èƒ½ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰**
   - å¦‚æžœé‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œè€ƒè™‘é™çº§transformers
   - å¯ä»¥å¯ç”¨gradient checkpointingèŠ‚çœæ˜¾å­˜
   - è°ƒæ•´batch sizeå’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°

3. **ä»£ç è´¨é‡**
   - æ·»åŠ æ›´å¤šçš„å‚æ•°éªŒè¯
   - å¢žåŠ å•å…ƒæµ‹è¯•è¦†ç›–
   - è€ƒè™‘æ·»åŠ æ¢¯åº¦ç›‘æŽ§å›žè°ƒ

---

**ä¿®å¤å®Œæˆæ—¶é—´ï¼š** 2025å¹´12æœˆ9æ—¥  
**ä¿®å¤äººå‘˜ï¼š** GitHub Copilot AI Assistant  
**éªŒè¯çŠ¶æ€ï¼š** ç­‰å¾…ç”¨æˆ·éªŒè¯

# å…³é”®å‘ç°: DeepSeek-OCR-Encoder çš„ @torch.inference_mode() è£…é¥°å™¨

## ğŸ”´ æ ¸å¿ƒé—®é¢˜

DeepSeek-OCR-Encoder åº“çš„**æ‰€æœ‰å‰å‘ä¼ æ’­æ–¹æ³•**éƒ½è¢« `@torch.inference_mode()` è£…é¥°,**å®Œå…¨é˜»æ–­äº†æ¢¯åº¦æµåŠ¨**ã€‚

## æºä»£ç åˆ†æ

æŸ¥çœ‹ https://github.com/dwojcik92/deepseek-ocr-encoder/blob/main/src/deepseek_ocr_encoder/encoder.py

### ç¬¬ 416 è¡Œ - å…¬å…± API
```python
@torch.inference_mode()
def encode(self, image):
    ...
    return self._encode_single_image(image)
```

### ç¬¬ 394 è¡Œ - å†…éƒ¨æ ¸å¿ƒæ–¹æ³• (å…³é”®!)
```python
@torch.inference_mode()  # <-- è¿™ä¹Ÿæœ‰è£…é¥°å™¨!
def _forward_core(self, x_nchw_channels_last: torch.Tensor) -> torch.Tensor:
    sam_out = self.sam(x_nchw_channels_last)
    B, C, Hs, Ws = sam_out.shape
    tokens = sam_out.flatten(2).transpose(1, 2).contiguous()
    # ... position embeddings ...
    tokens_plus = tokens + pos
    x_tok = self.clip_pre(tokens_plus)
    x_tok = self.clip_tr(x_tok)
    return tokens + x_tok
```

**é—®é¢˜**: å³ä½¿æˆ‘ä»¬ç»•è¿‡ `encode()` ç›´æ¥è°ƒç”¨ `_forward_core()`,æ¢¯åº¦ä»ç„¶ä¼šæ–­è£‚,å› ä¸º `_forward_core()` æœ¬èº«ä¹Ÿè¢«è£…é¥°äº†!

## @torch.inference_mode() çš„å½±å“

| è¡Œä¸º | `torch.no_grad()` | `@torch.inference_mode()` |
|------|-------------------|---------------------------|
| ç¦ç”¨æ¢¯åº¦è®¡ç®— | âœ“ | âœ“ |
| ç§»é™¤ autograd å…ƒæ•°æ® | âœ— | âœ“ |
| è¾“å‡ºå¤±å» grad_fn | âœ— | âœ“ |
| å¯é‡æ–°å¯ç”¨æ¢¯åº¦ | âœ“ | âœ— (ä¸å¯é€†) |
| æ€§èƒ½ | å¥½ | æ›´å¥½ |
| è®­ç»ƒåœºæ™¯å¯ç”¨ | âœ“ | âœ— |

## ä¸ºä»€ä¹ˆä¹‹å‰çš„ä¿®å¤å¤±è´¥

### âŒ å°è¯• 1: ç§»é™¤ä»£ç ä¸­çš„ torch.no_grad()
```python
# ç§»é™¤è¿™ä¸ª
# with torch.no_grad():
img_features = self.ocr_embed(img)
```
**ç»“æœ**: å¤±è´¥ - é—®é¢˜åœ¨åº“å†…éƒ¨,ä¸åœ¨æˆ‘ä»¬çš„ä»£ç 

### âŒ å°è¯• 2: ä¿®å¤ requires_grad=False
```python
# æ”¹ä¸º
padding = torch.zeros(..., requires_grad=True)
```
**ç»“æœ**: å¤±è´¥ - paddingä¸æ˜¯æ ¹æœ¬é—®é¢˜

### âŒ å°è¯• 3: ç›´æ¥è°ƒç”¨ _forward_core()
```python
img_features = self.ocr_embed._forward_core(x)
```
**ç»“æœ**: å¤±è´¥ - `_forward_core()` æœ¬èº«ä¹Ÿè¢«è£…é¥°äº†!

## âœ… æœ€ç»ˆè§£å†³æ–¹æ¡ˆ

**å®Œå…¨ç»•è¿‡æ‰€æœ‰è£…é¥°çš„æ–¹æ³•,ç›´æ¥æ‰‹åŠ¨è°ƒç”¨åº•å±‚ç»„ä»¶:**

```python
# é¢„å¤„ç†
x = self.ocr_embed._preproc_1024(img_pil).unsqueeze(0)
x = x.to(self.ocr_embed.device, dtype=self.ocr_embed.dtype)
x = x.to(memory_format=torch.channels_last)

# æ‰‹åŠ¨å®ç°å‰å‘ä¼ æ’­ (æ— è£…é¥°å™¨)
# Step 1: SAM encoder
sam_out = self.ocr_embed.sam(x)  # [1, 1024, Hs, Ws]
B, C, Hs, Ws = sam_out.shape

# Step 2: Flatten
tokens = sam_out.flatten(2).transpose(1, 2).contiguous()

# Step 3: Position embeddings
if Hs == 16 and Ws == 16:
    pos = self.ocr_embed._pos_fixed_16
else:
    # åŠ¨æ€è®¡ç®—
    import math
    table = self.ocr_embed.clip_pos_table
    grid_size = int(math.isqrt(table.size(0) - 1))
    base_grid = table[1: 1 + grid_size * grid_size].view(grid_size, grid_size, 1024)
    base_grid = base_grid.permute(2, 0, 1).unsqueeze(0)
    base_grid = torch.nn.functional.interpolate(base_grid, size=(Hs, Ws), mode="bicubic")
    pos = base_grid.flatten(2).transpose(1, 2).contiguous()

# Step 4: CLIP processing
tokens_plus = tokens + pos
x_tok = self.ocr_embed.clip_pre(tokens_plus)
x_tok = self.ocr_embed.clip_tr(x_tok)
img_features = tokens + x_tok  # [1, N, 1024] - ä¿æŒæ¢¯åº¦!
```

## ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ¡ˆæœ‰æ•ˆ?

1. **ç›´æ¥è®¿é—®åº•å±‚æ¨¡å—**: `sam`, `clip_pre`, `clip_tr` æ˜¯æ ‡å‡†çš„ `nn.Module`,æ²¡æœ‰è£…é¥°å™¨
2. **ä¿æŒæ¢¯åº¦æµ**: è™½ç„¶è¿™äº›æ¨¡å—çš„å‚æ•°è¢«å†»ç»“,ä½†æ¢¯åº¦ä»ç„¶å¯ä»¥æµç»å®ƒä»¬
3. **æ‰‹åŠ¨å®ç°**: æˆ‘ä»¬å¤åˆ¶äº† `_forward_core()` çš„é€»è¾‘,ä½†åœ¨æ­£å¸¸çš„ autograd ç¯å¢ƒä¸­æ‰§è¡Œ

## æ¢¯åº¦æµåŠ¨è·¯å¾„

```
è¾“å…¥å›¾åƒ
  â†“
é¢„å¤„ç† (transforms)
  â†“
self.ocr_embed.sam(x)           â† å†»ç»“å‚æ•°, ä½†æ¢¯åº¦æµè¿‡ âœ“
  â†“
flatten + transpose
  â†“
+ position embeddings
  â†“  
self.ocr_embed.clip_pre(...)    â† å†»ç»“å‚æ•°, ä½†æ¢¯åº¦æµè¿‡ âœ“
  â†“
self.ocr_embed.clip_tr(...)     â† å†»ç»“å‚æ•°, ä½†æ¢¯åº¦æµè¿‡ âœ“
  â†“
tokens + x_tok
  â†“
self.proj(...)                   â† å¯è®­ç»ƒ! æ¢¯åº¦ç´¯ç§¯ âœ“
  â†“
LLM forward
  â†“
Loss.backward()                  â† æ¢¯åº¦åå‘ä¼ æ’­ âœ“
```

## å…³é”®è¦ç‚¹

1. **å†»ç»“ â‰  é˜»æ–­**: å‚æ•°å†»ç»“ (`requires_grad=False`) ä¸æ„å‘³ç€åº”è¯¥é˜»æ–­æ¢¯åº¦æµ
2. **è£…é¥°å™¨é™·é˜±**: `@torch.inference_mode()` åœ¨æ‰€æœ‰æ–¹æ³•ä¸Š,åŒ…æ‹¬"å†…éƒ¨"æ–¹æ³•
3. **åº“è®¾è®¡**: DeepSeek-OCR-Encoder å®Œå…¨æ˜¯ä¸ºæ¨ç†è®¾è®¡çš„,ä¸æ”¯æŒè®­ç»ƒåœºæ™¯
4. **æºç å¿…çœ‹**: å¿…é¡»æŸ¥çœ‹å®é™…æºä»£ç æ‰èƒ½å‘ç°è¿™ä¸ªé—®é¢˜

## æ•™è®­

- ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“è¿›è¡Œè®­ç»ƒæ—¶,åŠ¡å¿…æ£€æŸ¥æ˜¯å¦æœ‰ `@torch.inference_mode()` æˆ–ç±»ä¼¼çš„æ¨ç†ä¼˜åŒ–
- ä¸è¦å‡è®¾"encoder"åº“æ”¯æŒè®­ç»ƒ,é™¤éæ–‡æ¡£æ˜ç¡®è¯´æ˜
- å½“é‡åˆ°ç¥ç§˜çš„æ¢¯åº¦é—®é¢˜æ—¶,ç›´æ¥æ£€æŸ¥æºä»£ç è€Œä¸æ˜¯ä¾èµ–æ–‡æ¡£

============================================================
MEMModel Adapter Training Script
============================================================
Â∑•‰ΩúÁõÆÂΩï: /mmu_nlp_ssd/tangjingyi03/OCR-MEM
Python: /mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python
PyTorch: 2.6.0+cu118
CUDA ÂèØÁî®: True
GPU Êï∞Èáè: 8
============================================================

ÂêØÂä® DeepSpeed Â§öÂç°ËÆ≠ÁªÉ...
GPU: 
GPU Êï∞Èáè: 8
WandB È°πÁõÆ: OCR-MEM

[2025-12-09 15:46:06,356] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-12-09 15:46:06,357] [INFO] [runner.py:630:main] cmd = /mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info run_training.py --base_model_path /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B --ocr_model_path /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR --train_data /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl --eval_data /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl --output_dir ./adapter_checkpoints --num_epochs 2 --batch_size 2 --gradient_accumulation 8 --learning_rate 2e-4 --wandb_project OCR-MEM --wandb_run_name mem_adapter_training
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.18.3-1+cuda12.1
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.18.3-1
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.18.3-1
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.18.3-1+cuda12.1
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-12-09 15:46:16,346] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.18.3-1
[2025-12-09 15:46:16,346] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-12-09 15:46:16,346] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-12-09 15:46:16,347] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-12-09 15:46:16,347] [INFO] [launch.py:180:main] dist_world_size=8
[2025-12-09 15:46:16,347] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-09 15:46:16,347] [INFO] [launch.py:272:main] process 1753061 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=0', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,348] [INFO] [launch.py:272:main] process 1753062 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=1', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,348] [INFO] [launch.py:272:main] process 1753063 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=2', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,349] [INFO] [launch.py:272:main] process 1753064 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=3', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,349] [INFO] [launch.py:272:main] process 1753065 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=4', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,350] [INFO] [launch.py:272:main] process 1753066 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=5', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,350] [INFO] [launch.py:272:main] process 1753067 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=6', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']
[2025-12-09 15:46:16,351] [INFO] [launch.py:272:main] process 1753068 spawned with command: ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=7', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training']

======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================

======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================

======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================




======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================


======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================


======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================


======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================


======================================================================
WARNING: Incompatible transformers version detected!
======================================================================
Current version: 4.57.3
Required version: >=4.30.0,<4.48.0

The DeepSeek-OCR model requires specific transformers features that
may not be available in version 4.57.3.

To fix this issue, please run:
  pip install 'transformers>=4.30.0,<4.48.0'

Recommended version: transformers==4.47.0
======================================================================


============================================================
============================================================

È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑÈ™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ

============================================================
============================================================


‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B

‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR

‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl

‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl


============================================================‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints


============================================================
============================================================


ÈÖçÁΩÆ DeepSpeedÈÖçÁΩÆ DeepSpeed
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================
============================================================
============================================================


‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================
============================================================
============================================================

DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàêDeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê


DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê========================================================================================================================


============================================================ÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.jsonÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json


ÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.jsonZeRO Stage: 2ZeRO Stage: 2


ZeRO Stage: 2CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅCPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅ


CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅÁ≤æÂ∫¶: BFloat16Á≤æÂ∫¶: BFloat16


Á≤æÂ∫¶: BFloat16ÂÖ®Â±Ä Batch Size: 128ÂÖ®Â±Ä Batch Size: 128


ÂÖ®Â±Ä Batch Size: 128ÊØè GPU Batch Size: 2ÊØè GPU Batch Size: 2


ÊØè GPU Batch Size: 2Ê¢ØÂ∫¶Á¥ØÁßØ: 8Ê¢ØÂ∫¶Á¥ØÁßØ: 8


Ê¢ØÂ∫¶Á¥ØÁßØ: 8World Size: 8World Size: 8


World Size: 8============================================================
============================================================



============================================================

============================================================

============================================================


============================================================ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞


ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞============================================================
============================================================



============================================================


============================================================
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================

‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================
DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê
============================================================
ÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json
ZeRO Stage: 2
CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅ
Á≤æÂ∫¶: BFloat16
ÂÖ®Â±Ä Batch Size: 128
ÊØè GPU Batch Size: 2
Ê¢ØÂ∫¶Á¥ØÁßØ: 8
World Size: 8
============================================================


============================================================
ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞
============================================================


============================================================
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================

‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================
DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê
============================================================
ÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json
ZeRO Stage: 2
CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅ
Á≤æÂ∫¶: BFloat16
ÂÖ®Â±Ä Batch Size: 128
ÊØè GPU Batch Size: 2
Ê¢ØÂ∫¶Á¥ØÁßØ: 8
World Size: 8
============================================================


============================================================
ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞
============================================================


============================================================
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================

‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================

‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================
È™åËØÅÈÖçÁΩÆÂíåË∑ØÂæÑ
============================================================

‚úì Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B
‚úì OCR Ê®°ÂûãË∑ØÂæÑ: /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR
‚úì ËÆ≠ÁªÉÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÊï∞ÊçÆ: /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËæìÂá∫ÁõÆÂΩï: ./adapter_checkpoints

============================================================
ÈÖçÁΩÆ DeepSpeed
============================================================

============================================================

============================================================DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê

============================================================DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàê
============================================================

ÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json============================================================


ZeRO Stage: 2DeepSpeed ÈÖçÁΩÆÂ∑≤ÁîüÊàêÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json


CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅ============================================================ZeRO Stage: 2


Á≤æÂ∫¶: BFloat16CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅÈÖçÁΩÆÊñá‰ª∂: ./adapter_checkpoints/ds_config_auto.json


ÂÖ®Â±Ä Batch Size: 128Á≤æÂ∫¶: BFloat16ZeRO Stage: 2


ÊØè GPU Batch Size: 2ÂÖ®Â±Ä Batch Size: 128CPU Offloading: ‰ºòÂåñÂô®Áä∂ÊÄÅ


Ê¢ØÂ∫¶Á¥ØÁßØ: 8ÊØè GPU Batch Size: 2Á≤æÂ∫¶: BFloat16


World Size: 8Ê¢ØÂ∫¶Á¥ØÁßØ: 8ÂÖ®Â±Ä Batch Size: 128


============================================================

World Size: 8ÊØè GPU Batch Size: 2


========================================================================================================================
Ê¢ØÂ∫¶Á¥ØÁßØ: 8


ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞World Size: 8

============================================================
============================================================

============================================================

ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞

============================================================

============================================================

ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞
============================================================


‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 148.36it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 145.98it/s]
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.

‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 144.99it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 149.81it/s]

‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...

============================================================
ÂàùÂßãÂåñ WandB
============================================================


‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...

‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 148.73it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 142.82it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 147.78it/s]
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: Currently logged in as: kscolor0523 (kscolor0523-kuaishou) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: setting up run 2rxqvkpm
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /mmu_nlp_ssd/tangjingyi03/OCR-MEM/wandb/run-20251209_154637-2rxqvkpm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mem_adapter_training
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kscolor0523-kuaishou/OCR-MEM
wandb: üöÄ View run at https://wandb.ai/kscolor0523-kuaishou/OCR-MEM/runs/2rxqvkpm
‚úì WandB È°πÁõÆ: OCR-MEM
‚úì WandB ÂÆûÈ™å: mem_adapter_training

‚úì ËÆ≠ÁªÉËΩÆÊï∞: 2
‚úì ÊØè GPU Batch Size: 2
‚úì Ê¢ØÂ∫¶Á¥ØÁßØ: 8
‚úì Â≠¶‰π†Áéá: 0.0002

============================================================
ÂàùÂßãÂåñÊ®°Âûã
============================================================

Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºåËøôÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 147.42it/s]
You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.
Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)

============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
‚úì Ê®°ÂûãÂä†ËΩΩÂÆåÊàê
trainable params: 18,882,560 || all params: 8,607,498,496 || trainable%: 0.2194%

Detailed breakdown:
  base_llm_model trainable params: 0 (should be 0)
  ocr_embed trainable params: 0 (should be 0)
  proj trainable params: 18,882,560

============================================================
Âä†ËΩΩÊï∞ÊçÆÈõÜ
============================================================

Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì ËÆ≠ÁªÉÈõÜ: 2549 Ê†∑Êú¨
Loaded 2549 samples from /mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl
‚úì È™åËØÅÈõÜ: 2549 Ê†∑Êú¨

============================================================
ÂàùÂßãÂåñËÆ≠ÁªÉÂô®
============================================================


============================================================
ÂèÇÊï∞ÁªüËÆ° (Parameter Statistics)
============================================================
ÂèØËÆ≠ÁªÉÂèÇÊï∞ (Trainable):     18,882,560
ÊÄªÂèÇÊï∞ (Total):             8,607,498,496
ÂèØËÆ≠ÁªÉÊØî‰æã (Trainable %):   0.2194%
============================================================

/mmu_nlp_ssd/tangjingyi03/OCR-MEM/trainer/adapter_only_trainer.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AdapterOnlyTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
‚úì ËÆ≠ÁªÉÂô®ÂàùÂßãÂåñÂÆåÊàê

============================================================
ÂºÄÂßãËÆ≠ÁªÉ
============================================================

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[2025-12-09 15:47:53,026] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,026] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,026] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,027] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,027] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,028] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,032] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-09 15:47:53,631] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
WARNING:root:base_model_name is empty, model may not initialize correctly
WARNING:root:ocr_model_name is empty, model may not initialize correctly
  0%|          | 0/40 [00:00<?, ?it/s][rank4]: Traceback (most recent call last):
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank4]:     main()
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank4]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank4]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank4]:     self.engine.backward(loss, **kwargs)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank4]:     self._do_optimizer_backward(loss, retain_graph)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank4]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank4]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank4]:     scaled_loss.backward(retain_graph=retain_graph)
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank5]:     main()
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank5]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank5]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank5]:     self.engine.backward(loss, **kwargs)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:               ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank5]:     self._do_optimizer_backward(loss, retain_graph)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank5]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank5]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank5]:     scaled_loss.backward(retain_graph=retain_graph)
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank2]:     main()
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank2]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank2]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank2]:     self.engine.backward(loss, **kwargs)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank2]:     self._do_optimizer_backward(loss, retain_graph)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank2]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank2]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank2]:     scaled_loss.backward(retain_graph=retain_graph)
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank1]:     main()
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank1]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank1]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank1]:     self.engine.backward(loss, **kwargs)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank1]:     self._do_optimizer_backward(loss, retain_graph)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank1]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank1]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank1]:     scaled_loss.backward(retain_graph=retain_graph)
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank7]:     main()
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank7]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank7]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank7]:     self.engine.backward(loss, **kwargs)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:               ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank7]:     self._do_optimizer_backward(loss, retain_graph)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank7]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank7]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank7]:     scaled_loss.backward(retain_graph=retain_graph)
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank3]:     main()
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank3]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank3]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank3]:     self.engine.backward(loss, **kwargs)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank3]:     self._do_optimizer_backward(loss, retain_graph)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank3]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank3]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank3]:     scaled_loss.backward(retain_graph=retain_graph)
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank6]:     main()
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank6]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank6]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank6]:     self.engine.backward(loss, **kwargs)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:               ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank6]:     self._do_optimizer_backward(loss, retain_graph)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank6]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank6]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank6]:     scaled_loss.backward(retain_graph=retain_graph)
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
    main()
  File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
    self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
    self.engine.backward(loss, **kwargs)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
    self._do_optimizer_backward(loss, retain_graph)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 797, in <module>
[rank0]:     main()
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/run_training.py", line 764, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/accelerator.py", line 2844, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2356, in backward
[rank0]:     self._do_optimizer_backward(loss, retain_graph)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2297, in _do_optimizer_backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2245, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 65, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[2025-12-09 15:47:58,360] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753061
[2025-12-09 15:47:59,667] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753062
[2025-12-09 15:47:59,672] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753063
[2025-12-09 15:47:59,675] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753064
[2025-12-09 15:47:59,675] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753065
[2025-12-09 15:47:59,678] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753066
[2025-12-09 15:47:59,682] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753067
[2025-12-09 15:47:59,685] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 1753068
[2025-12-09 15:47:59,688] [ERROR] [launch.py:341:sigkill_handler] ['/mmu_nlp_ssd/tangjingyi03/miniconda/envs/mem_new/bin/python3.12', '-u', 'run_training.py', '--local_rank=7', '--base_model_path', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B', '--ocr_model_path', '/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR', '--train_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--eval_data', '/mmu_nlp_ssd/tangjingyi03/OCR-MEM/data/train_data_of_merged_total_66025_rmtestset_first_100.jsonl', '--output_dir', './adapter_checkpoints', '--num_epochs', '2', '--batch_size', '2', '--gradient_accumulation', '8', '--learning_rate', '2e-4', '--wandb_project', 'OCR-MEM', '--wandb_run_name', 'mem_adapter_training'] exits with return code = 1

============================================================
‚ùå ËÆ≠ÁªÉÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•ÈîôËØØÊó•Âøó
============================================================

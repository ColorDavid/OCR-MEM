#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€ŸéªŒè¯æ¢¯åº¦æµåŠ¨è„šæœ¬
ç”¨äºç¡®è®¤ä¿®å¤åçš„æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®ä¼ æ’­æ¢¯åº¦
"""

import sys
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from mem_adapter_only import MEMModel, MEMConfig

def verify_gradient_flow():
    """éªŒè¯æ¢¯åº¦æµåŠ¨æ˜¯å¦æ­£å¸¸"""
    
    print("=" * 70)
    print("æ¢¯åº¦æµåŠ¨éªŒè¯è„šæœ¬")
    print("=" * 70)
    
    # é…ç½®å‚æ•° - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è·¯å¾„
    base_model_path = input("è¯·è¾“å…¥åŸºç¡€æ¨¡å‹è·¯å¾„ (æˆ–æŒ‰Enterä½¿ç”¨é»˜è®¤): ").strip()
    ocr_model_path = input("è¯·è¾“å…¥OCRæ¨¡å‹è·¯å¾„ (æˆ–æŒ‰Enterä½¿ç”¨é»˜è®¤): ").strip()
    
    if not base_model_path:
        base_model_path = "/mmu_nlp_ssd/tangjingyi03/OCR-MEM/model/Qwen/Qwen3-8B"
    if not ocr_model_path:
        ocr_model_path = "/mmu_nlp_ssd/tangjingyi03/models/deepseek-ai/DeepSeek-OCR"
    
    print(f"\nä½¿ç”¨æ¨¡å‹è·¯å¾„:")
    print(f"  Base Model: {base_model_path}")
    print(f"  OCR Model:  {ocr_model_path}")
    
    try:
        # 1. åˆ›å»ºé…ç½®
        print("\n[1/6] åˆ›å»ºæ¨¡å‹é…ç½®...")
        config = MEMConfig(
            base_model_name=base_model_path,
            ocr_model_name=ocr_model_path,
            vision_embedding_size=1024,
            context_threshold=2048
        )
        print("âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # 2. åŠ è½½æ¨¡å‹
        print("\n[2/6] åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        model = MEMModel(config)
        model.train()  # ç¡®ä¿åœ¨è®­ç»ƒæ¨¡å¼
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. æ‰“å°å‚æ•°ç»Ÿè®¡
        print("\n[3/6] éªŒè¯å‚æ•°ç»Ÿè®¡...")
        model.print_trainable_parameters()
        
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        if trainable_params == 0:
            print("âŒ é”™è¯¯ï¼šæ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")
            return False
        print("âœ… å‚æ•°ç»Ÿè®¡æ­£ç¡®")
        
        # 4. åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        print("\n[4/6] åˆ›å»ºè™šæ‹Ÿè¾“å…¥...")
        batch_size = 1
        seq_len = 100
        vocab_size = model.tokenizer.vocab_size
        
        dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len))
        dummy_attention_mask = torch.ones_like(dummy_input)
        dummy_labels = torch.randint(0, vocab_size, (batch_size, seq_len))
        
        # å°†ä¸€äº›ä½ç½®è®¾ä¸º-100ï¼ˆä¸è®¡ç®—lossï¼‰
        dummy_labels[:, :50] = -100
        print("âœ… è™šæ‹Ÿè¾“å…¥åˆ›å»ºæˆåŠŸ")
        
        # 5. å‰å‘ä¼ æ’­
        print("\n[5/6] æ‰§è¡Œå‰å‘ä¼ æ’­...")
        outputs = model(
            input_ids=dummy_input,
            attention_mask=dummy_attention_mask,
            labels=dummy_labels
        )
        
        # æ£€æŸ¥loss
        if outputs.loss is None:
            print("âŒ é”™è¯¯ï¼šlossä¸ºNoneï¼")
            return False
        
        if not outputs.loss.requires_grad:
            print("âŒ é”™è¯¯ï¼šlossæ²¡æœ‰requires_gradï¼")
            return False
        
        if outputs.loss.grad_fn is None:
            print("âŒ é”™è¯¯ï¼šlossæ²¡æœ‰grad_fnï¼")
            print("   è¿™è¡¨æ˜æ¢¯åº¦è®¡ç®—å›¾è¢«æ–­å¼€äº†ã€‚")
            return False
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   Losså€¼: {outputs.loss.item():.4f}")
        print(f"   Loss requires_grad: {outputs.loss.requires_grad}")
        print(f"   Loss grad_fn: {outputs.loss.grad_fn}")
        
        # 6. åå‘ä¼ æ’­
        print("\n[6/6] æ‰§è¡Œåå‘ä¼ æ’­...")
        outputs.loss.backward()
        print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
        # 7. éªŒè¯æ¢¯åº¦
        print("\néªŒè¯æ¢¯åº¦åˆ†å¸ƒ...")
        proj_has_grad = False
        base_has_grad = False
        ocr_has_grad = False
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                if param.grad is None:
                    print(f"âŒ è­¦å‘Šï¼š{name} éœ€è¦æ¢¯åº¦ä½†gradä¸ºNone")
                else:
                    if 'proj' in name:
                        proj_has_grad = True
                        print(f"âœ… projå‚æ•°æœ‰æ¢¯åº¦: {name}")
                        print(f"   æ¢¯åº¦ç»Ÿè®¡: mean={param.grad.mean().item():.6f}, "
                              f"std={param.grad.std().item():.6f}, "
                              f"max={param.grad.abs().max().item():.6f}")
            else:
                # å†»ç»“å‚æ•°ä¸åº”è¯¥æœ‰æ¢¯åº¦
                if param.grad is not None:
                    if 'base_llm_model' in name:
                        base_has_grad = True
                        print(f"âŒ é”™è¯¯ï¼šbase_llmå‚æ•°ä¸åº”è¯¥æœ‰æ¢¯åº¦: {name}")
                    elif 'ocr_embed' in name:
                        ocr_has_grad = True
                        print(f"âŒ é”™è¯¯ï¼šocr_embedå‚æ•°ä¸åº”è¯¥æœ‰æ¢¯åº¦: {name}")
        
        # æœ€ç»ˆéªŒè¯
        print("\n" + "=" * 70)
        print("éªŒè¯ç»“æœæ±‡æ€»")
        print("=" * 70)
        
        all_passed = True
        
        if not proj_has_grad:
            print("âŒ å¤±è´¥ï¼šprojå±‚æ²¡æœ‰æ¥æ”¶åˆ°æ¢¯åº¦")
            all_passed = False
        else:
            print("âœ… é€šè¿‡ï¼šprojå±‚æ­£ç¡®æ¥æ”¶æ¢¯åº¦")
        
        if base_has_grad:
            print("âŒ å¤±è´¥ï¼šbase_llmæ¨¡å‹æœ‰æ¢¯åº¦ï¼ˆåº”è¯¥è¢«å†»ç»“ï¼‰")
            all_passed = False
        else:
            print("âœ… é€šè¿‡ï¼šbase_llmæ¨¡å‹æ­£ç¡®å†»ç»“")
        
        if ocr_has_grad:
            print("âŒ å¤±è´¥ï¼šocr_embedæœ‰æ¢¯åº¦ï¼ˆåº”è¯¥è¢«å†»ç»“ï¼‰")
            all_passed = False
        else:
            print("âœ… é€šè¿‡ï¼šocr_embedæ­£ç¡®å†»ç»“")
        
        print("=" * 70)
        
        if all_passed:
            print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼æ¢¯åº¦æµåŠ¨æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
            return True
        else:
            print("âš ï¸  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ã€‚")
            return False
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š")
        print(f"   {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = verify_gradient_flow()
    sys.exit(0 if success else 1)
